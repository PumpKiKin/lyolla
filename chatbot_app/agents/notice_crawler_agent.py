import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from pathlib import Path
import pandas as pd
import re
import json


class NoticeCrawlerAgent:
    BASE_URL = "https://library.sogang.ac.kr"
    NOTICE_URL = "https://library.sogang.ac.kr/bbs/list/1"
    HEADERS = {"User-Agent": "Mozilla/5.0 (compatible; SogangNoticeBot/1.0)"}

    def __init__(self, output_dir: str = "database"):
        self.output_file = Path(output_dir) / "notices.json"
        self.output_file.parent.mkdir(parents=True, exist_ok=True)

    # === í¬ë¡¤ë§ ë©”ì„œë“œ ===
    def fetch_notices(self) -> pd.DataFrame:
        """ê³µì§€ì‚¬í•­ ëª©ë¡ í¬ë¡¤ë§"""
        all_data = []
        page = 1
        stop_crawling = False

        while not stop_crawling:
            url = f"{self.NOTICE_URL}?pn={page}"
            resp = requests.get(url, headers=self.HEADERS)
            resp.raise_for_status()
            soup = BeautifulSoup(resp.text, "html.parser")

            rows = soup.select("table tbody tr")
            if not rows:
                break

            for tr in rows:
                cols = tr.find_all("td")
                if len(cols) < 5:
                    continue

                no = cols[0].get_text(strip=True)
                title_cell = cols[1]
                a = title_cell.find("a")
                title = a.get_text(strip=True) if a else title_cell.get_text(strip=True)
                href = urljoin(self.BASE_URL, a["href"]) if a and a.has_attr("href") else self.NOTICE_URL
                author = cols[2].get_text(strip=True)
                date = cols[3].get_text(strip=True)
                views = cols[4].get_text(strip=True)

                # 2024 ë˜ëŠ” 2025ë§Œ ê°€ì ¸ì˜¤ê¸°
                if not (date.startswith("2024") or date.startswith("2025")):
                    stop_crawling = True
                    break

                all_data.append({
                    "No.": no,
                    "ì œëª©": title,
                    "ì‘ì„±ì": author,
                    "ì‘ì„±ì¼": date,
                    "ì¡°íšŒìˆ˜": views,
                    "ë§í¬": href
                })

            page += 1

        return pd.DataFrame(all_data)

    def fetch_notice_detail(self, url: str) -> dict:
        """ê³µì§€ì‚¬í•­ ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§"""
        r = requests.get(url, headers=self.HEADERS, timeout=10)
        r.raise_for_status()
        s = BeautifulSoup(r.text, "html.parser")

        content =   s.select_one(".boardContent") or \
                    s.select_one(".board-view .board-txt") or \
                    s.select_one(".bbs_view .view_con") or \
                    s.select_one(".contents") or \
                    s.select_one("article") or \
                    s.select_one("#content")

        body_parts = []
        if content:
            for img in content.find_all("img"):
                img_url = urljoin(self.BASE_URL, img.get("src"))
                body_parts.append(f"![ì´ë¯¸ì§€]({img_url})")

            text_only = self.clean_notice_content(content)
            if text_only:
                body_parts.append(text_only)

        body_text = "\n".join(body_parts)

        # ì²¨ë¶€íŒŒì¼
        atts = []
        for a in s.select('a[href]'):
            href = a["href"]
            if any(k in href.lower() for k in ["download", "attach", "file", "files"]):
                atts.append({"name": a.get_text(strip=True), "url": urljoin(self.BASE_URL, href)})

        seen = set()
        attachments = []
        for x in atts:
            if x["url"] not in seen:
                seen.add(x["url"])
                attachments.append(x)

        title = (s.select_one("h3, h2, .title, .board-tit") or content)
        title_text = title.get_text(strip=True) if title else ""

        return {
            "url": url,
            "title": title_text,
            "body": body_text,
            "attachments": attachments,
        }

    def clean_notice_content(self, content):
        """ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì •ë¦¬"""
        for br in content.find_all("br"):
            br.replace_with("\n")
        text = content.get_text()
        clean_text = re.sub(r'\n{3,}', '\n\n', text)
        return clean_text.strip()

    # === JSON ì €ì¥ ===
    def create_notices_json(self):
        """ê³µì§€ì‚¬í•­ ëª©ë¡ + ìƒì„¸ ë‚´ìš©ì„ JSONìœ¼ë¡œ ì €ì¥"""
        print("ğŸ“¢ ê³µì§€ì‚¬í•­ ëª©ë¡ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
        notice_df = self.fetch_notices()

        if notice_df.empty:
            print("ê°€ì ¸ì˜¬ ê³µì§€ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        all_notices = []
        for _, row in notice_df.iterrows():
            url = row["ë§í¬"]
            try:
                print(f"â¡ï¸ ìƒì„¸ ë‚´ìš© ê°€ì ¸ì˜¤ëŠ” ì¤‘: {url}")
                notice_detail = self.fetch_notice_detail(url)

                notice_data = {
                    "source": url,
                    "title": row["ì œëª©"],
                    "author": row["ì‘ì„±ì"],
                    "date": row["ì‘ì„±ì¼"],
                    "content": notice_detail["body"]
                }
                all_notices.append(notice_data)

            except Exception as e:
                print(f"âŒ ìƒì„¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {url}, ì˜¤ë¥˜: {e}")
                continue

        with open(self.output_file, 'w', encoding='utf-8') as f:
            json.dump(all_notices, f, ensure_ascii=False, indent=4)
        print(f"âœ… ê³µì§€ì‚¬í•­ ë°ì´í„°ê°€ '{self.output_file}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # === ì—ì´ì „íŠ¸ ì‹¤í–‰ ===
    def run(self):
        """Agent ì‹¤í–‰"""
        self.create_notices_json()
        return f"âœ… ê³µì§€ì‚¬í•­ ë°ì´í„°ê°€ {self.output_file} ì— ì €ì¥ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."


if __name__ == "__main__":
    agent = NoticeCrawlerAgent()
    agent.run()
