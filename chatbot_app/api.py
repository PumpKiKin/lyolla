from pathlib import Path
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from langchain.schema.output_parser import StrOutputParser
import os
from dotenv import load_dotenv
from .embeddings import build_faiss_index

# .env ë¡œë“œ
load_dotenv()

BASE_DIR = Path(__file__).resolve().parent.parent
DB_PATH = BASE_DIR / "faiss_index"

def load_vectorstore():
    # âœ… ë²¡í„°ìŠ¤í† ì–´ê°€ ì—†ìœ¼ë©´ ìë™ ìƒì„±
    if not DB_PATH.exists():
        print("âš ï¸ FAISS ì¸ë±ìŠ¤ê°€ ì—†ì–´ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤...")
        build_faiss_index()
        
    embeddings = HuggingFaceEmbeddings(
        model_name="jhgan/ko-sbert-nli",
        model_kwargs={"device": "cpu"})
    return FAISS.load_local(str(DB_PATH), embeddings, allow_dangerous_deserialization=True)

def process_question(user_question, history):
    db = load_vectorstore()
    retriever = db.as_retriever(search_kwargs={"k": 5})
    docs = retriever.invoke(user_question)

    # format history
    history_text = "\n".join([f"{h['role']}: {h['content']}" for h in history[-8:]])

    template = """
    ë„ˆëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ëŠ” ì„œê°•ëŒ€í•™êµ ë¡œìšœë¼ë„ì„œê´€ ë„ìš°ë¯¸ì•¼.
    ì•„ë˜ ì»¨í…ìŠ¤íŠ¸ì™€ ì´ì „ ëŒ€í™”ë¥¼ ì°¸ê³ í•´ì„œ, í•„ìš”í•œ ì •ë³´ë§Œ ì •ë¦¬í•´ì„œ ë‹µë³€í•´.
    ì¤‘ë³µëœ í‘œí˜„ì€ í”¼í•˜ê³ , ë¶ˆí™•ì‹¤í•œ ë¶€ë¶„ì€ 'ì œê³µëœ ì •ë³´ë¡œëŠ” ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µí•´.

    ì¶œë ¥ ê·œì¹™:
    1. í•­ìƒ `### ğŸ” ìƒì„¸ ì„¤ëª…` ì„¹ì…˜ì„ ì¶œë ¥í•˜ë˜, ì œëª©ì€ ì¶œë ¥í•˜ì§€ ë§ ê²ƒ.
    2. ë§Œì•½ ìƒì„¸ ì„¤ëª…ì´ ë„ˆë¬´ ê¸¸ë‹¤ë©´, **ê·¸ë•Œë§Œ** `### âœï¸ í•µì‹¬ ìš”ì•½` ì„¹ì…˜ì„ í•¨ê»˜ ì¶œë ¥. ì´ë•ŒëŠ” ì œëª©ì„ ì¶œë ¥í•  ê²ƒ.
    (í•µì‹¬ ìš”ì•½ì´ í•„ìš”í•˜ì§€ ì•Šìœ¼ë©´ ì„¹ì…˜ ìì²´ë¥¼ ì¶œë ¥í•˜ì§€ ë§ ê²ƒ.)
    3. í•­ìƒ `### ğŸ”— ê´€ë ¨ ë§í¬` ì„¹ì…˜ì„ ì¶œë ¥. ë§í¬ëŠ” HTML `<a>` íƒœê·¸ë¡œ ì‘ì„±.

    ---

    ì´ì „ ëŒ€í™”: {history}

    ì»¨í…ìŠ¤íŠ¸: {context}

    ì§ˆë¬¸: {question}

    ì‘ë‹µ:
    """

    prompt = PromptTemplate.from_template(template)
    model = ChatGoogleGenerativeAI(model="gemini-2.5-flash")
    chain = prompt | model | StrOutputParser()
    response = chain.invoke({
        "question": user_question,
        "context": docs,
        "history": history_text
    })

    return {"answer": response, "sources": [d.metadata.get("source","") for d in docs]}
